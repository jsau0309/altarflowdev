# Health Check Notification Fix

## Issue Identified

**Location**: `app/api/health/clerk/route.ts` (lines 167-176)

**Problem**: The comment claimed Slack notifications were only sent on first failure, but the code actually sent a notification **every time** the cache expired (every 4 minutes) during an outage. This would spam Slack with repeated failure notifications instead of just sending one notification when the service first goes down.

### Root Cause

The original code did this:
```typescript
// Send Slack notification for authentication system failure
// Only send if this is the first failure (not a cached failure)  â† COMMENT WAS WRONG
await sendSlackNotification(
  SlackNotifications.serviceHealthCheckFailed({
    service: 'clerk',
    error: errorMessage,
    responseTime: `${responseTime}ms`,
  })
);
```

The code sent a notification on **every cache miss** when the API call failed, not just on the first failure. Since the cache expires every 4 minutes, this would send:
- Check 1 (healthy): âœ… No notification
- Check 2 (unhealthy): ğŸ”” Notification #1
- Check 3 (unhealthy): ğŸ”” Notification #2 âŒ SPAM
- Check 4 (unhealthy): ğŸ”” Notification #3 âŒ SPAM
- Check 5 (unhealthy): ğŸ”” Notification #4 âŒ SPAM
- ...continues every 4 minutes...

## Solution Implemented

Added state transition tracking to send notifications **only when the service changes state** (healthy â†’ unhealthy or unhealthy â†’ healthy).

### Changes Made

#### 1. Added State Tracking (`app/api/health/clerk/route.ts`)

```typescript
let lastNotificationStatus: 'healthy' | 'unhealthy' | null = null; // Track last notified state
```

#### 2. Fixed Failure Notification Logic

```typescript
// Send Slack notification ONLY on transition from healthy to unhealthy
// This prevents spam during extended outages (when cache expires every 4 minutes)
const wasHealthy = healthCheckCache?.status === 'healthy' || healthCheckCache === null;

if (wasHealthy && lastNotificationStatus !== 'unhealthy') {
  await sendSlackNotification(
    SlackNotifications.serviceHealthCheckFailed({
      service: 'clerk',
      error: errorMessage,
      responseTime: `${responseTime}ms`,
    })
  );
  lastNotificationStatus = 'unhealthy';
  
  logger.warn('Clerk health check transitioned to unhealthy - notification sent', {
    operation: 'health.clerk.failure_notification',
    responseTime,
  });
} else {
  logger.debug('Clerk health check still unhealthy - notification suppressed', {
    operation: 'health.clerk.failure_suppressed',
    responseTime,
    lastNotificationStatus,
  });
}
```

#### 3. Added Recovery Notification Logic

```typescript
const wasUnhealthy = healthCheckCache?.status === 'unhealthy';

// Send recovery notification only if transitioning from unhealthy to healthy
if (wasUnhealthy && lastNotificationStatus === 'unhealthy') {
  await sendSlackNotification(
    SlackNotifications.serviceHealthCheckRecovered({
      service: 'clerk',
      responseTime: `${responseTime}ms`,
    })
  );
  lastNotificationStatus = 'healthy';
  
  logger.info('Clerk health check recovered - notification sent', {
    operation: 'health.clerk.recovery',
    responseTime,
  });
}
```

#### 4. Added Recovery Notification Template (`lib/slack-notifier.ts`)

```typescript
/**
 * Third-party service health check recovered
 */
serviceHealthCheckRecovered: (details: {
  service: 'supabase' | 'stripe' | 'clerk' | 'resend';
  responseTime?: string;
}): SlackNotification => {
  const serviceLabels = {
    supabase: { name: 'Supabase Database', emoji: 'ğŸ—„ï¸' },
    stripe: { name: 'Stripe Payments', emoji: 'ğŸ’³' },
    clerk: { name: 'Clerk Authentication', emoji: 'ğŸ”' },
    resend: { name: 'Resend Email', emoji: 'ğŸ“§' },
  };

  const serviceInfo = serviceLabels[details.service];

  return {
    title: `âœ… ${serviceInfo.name} Health Check Recovered`,
    message: `The ${serviceInfo.name} integration has recovered and is now operational.`,
    severity: 'info',
    fields: [
      { title: 'Service', value: serviceInfo.name, short: true },
      { title: 'Response Time', value: details.responseTime || 'N/A', short: true },
      { title: 'Status', value: 'Service is now healthy', short: false },
    ],
  };
},
```

## Expected Behavior After Fix

- Check 1 (healthy): âœ… No notification
- Check 2 (unhealthy): ğŸ”” **Notification #1** (First failure)
- Check 3 (unhealthy): ğŸ”• Suppressed (already notified)
- Check 4 (unhealthy): ğŸ”• Suppressed (already notified)
- Check 5 (unhealthy): ğŸ”• Suppressed (already notified)
- Check 6 (healthy): ğŸ”” **Notification #2** (Recovery)
- Check 7 (healthy): âœ… No notification
- Check 8 (unhealthy): ğŸ”” **Notification #3** (New failure)

**Result**: Only 3 notifications instead of 6+ during an outage scenario.

## Verification

Run the test script to verify the fix:

```bash
node --import tsx scripts/test-health-check-notifications.ts
```

Expected output:
```
ğŸ‰ All validations passed!
âœ“ Notifications only sent on state transitions
âœ“ No spam during extended outages
âœ“ Recovery notifications sent when service comes back
```

## Benefits

1. **No Spam**: Notifications sent only on state changes, not every cache expiration
2. **Better Alerting**: Clear distinction between first failure and continued failures
3. **Recovery Awareness**: Team is notified when service comes back online
4. **Actionable Logs**: All notification decisions are logged with context

## Files Modified

- `app/api/health/clerk/route.ts` - Fixed notification logic with state tracking
- `lib/slack-notifier.ts` - Added recovery notification template
- `scripts/test-health-check-notifications.ts` - Created test to verify behavior

## Related Documentation

- `docs/CACHE_AND_MONITORING_SUMMARY.md` - Overview of caching strategy
- `docs/CACHE_TESTING_GUIDE.md` - Testing guide for health checks

## Impact

This fix applies to the **Clerk health check endpoint** which is called every 5 minutes by uptime monitoring.

### Other Health Check Endpoints

The following endpoints also send Slack notifications on failure but **do not have caching**:
- `app/api/health/supabase/route.ts` - Supabase database health check
- `app/api/health/stripe/route.ts` - Stripe payments health check  
- `app/api/health/resend/route.ts` - Resend email health check

**Analysis**: These endpoints are likely only called when there's an actual issue (not every 5 minutes like Clerk), so they may not have the same spam problem. However, if they are ever integrated with uptime monitoring or called frequently, they should be updated with the same state transition logic.

**Recommendation**: Monitor these endpoints in production. If they show signs of notification spam, apply the same fix pattern (state tracking + transition detection).

## Next Steps

1. âœ… Review and verify the fix
2. âœ… Run test script
3. â³ Monitor Clerk Slack notifications in production to confirm fix works
4. â³ Review other health check endpoints if they get integrated with uptime monitoring
5. â³ Consider adding caching to other health check endpoints (similar to Clerk)
6. â³ Consider adding metrics for notification frequency and health check results

